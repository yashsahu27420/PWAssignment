{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "gmcK6row4HgX",
        "outputId": "ccd3f6f5-5cb0-4f1e-af9b-af615d0dd25a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nBayes' theorem is a fundamental theorem in probability theory and statistics that describes how to update the probability of a hypothesis (an educated guess or proposition) based on new evidence. It is named after Thomas Bayes, an 18th-century statistician and theologian, although the theorem was initially developed by Pierre-Simon Laplace.\\n\\nMathematically, Bayes' theorem is expressed as follows:\\n\\n\\\\[ P(A|B) = \\x0crac{P(B|A) \\\\cdot P(A)}{P(B)} \\\\]\\n\\nWhere:\\n- \\\\(P(A|B)\\\\) is the posterior probability of hypothesis A given evidence B.\\n- \\\\(P(B|A)\\\\) is the probability of evidence B given hypothesis A.\\n- \\\\(P(A)\\\\) is the prior probability of hypothesis A (the probability of A before considering the evidence).\\n- \\\\(P(B)\\\\) is the probability of evidence B.\\n\\nIn essence, Bayes' theorem allows you to update your belief in a hypothesis (A) based on new evidence (B) by considering both the prior probability of the hypothesis and the likelihood of the evidence under that hypothesis.\\n\\nBayes' theorem is widely used in various fields, including statistics, machine learning, Bayesian inference, and artificial intelligence. It plays a crucial role in decision-making, classification problems, and probabilistic reasoning, among other applications.\\n\\nThe theorem is the foundation for Bayesian statistics, which is a statistical framework that incorporates prior knowledge and data to make probabilistic inferences about unknown parameters or hypotheses.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Q1. What is Bayes' theorem?\n",
        "\n",
        "'''\n",
        "Bayes' theorem is a fundamental theorem in probability theory and statistics that describes how to update the probability of a hypothesis (an educated guess or proposition) based on new evidence. It is named after Thomas Bayes, an 18th-century statistician and theologian, although the theorem was initially developed by Pierre-Simon Laplace.\n",
        "\n",
        "Mathematically, Bayes' theorem is expressed as follows:\n",
        "\n",
        "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
        "\n",
        "Where:\n",
        "- \\(P(A|B)\\) is the posterior probability of hypothesis A given evidence B.\n",
        "- \\(P(B|A)\\) is the probability of evidence B given hypothesis A.\n",
        "- \\(P(A)\\) is the prior probability of hypothesis A (the probability of A before considering the evidence).\n",
        "- \\(P(B)\\) is the probability of evidence B.\n",
        "\n",
        "In essence, Bayes' theorem allows you to update your belief in a hypothesis (A) based on new evidence (B) by considering both the prior probability of the hypothesis and the likelihood of the evidence under that hypothesis.\n",
        "\n",
        "Bayes' theorem is widely used in various fields, including statistics, machine learning, Bayesian inference, and artificial intelligence. It plays a crucial role in decision-making, classification problems, and probabilistic reasoning, among other applications.\n",
        "\n",
        "The theorem is the foundation for Bayesian statistics, which is a statistical framework that incorporates prior knowledge and data to make probabilistic inferences about unknown parameters or hypotheses.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. What is the formula for Bayes' theorem?\n",
        "\n",
        "'''\n",
        "The formula for Bayes' theorem is as follows:\n",
        "\n",
        "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\(P(A|B)\\) is the posterior probability of hypothesis A given evidence B.\n",
        "- \\(P(B|A)\\) is the probability of evidence B given hypothesis A.\n",
        "- \\(P(A)\\) is the prior probability of hypothesis A (the probability of A before considering the evidence).\n",
        "- \\(P(B)\\) is the probability of evidence B.\n",
        "\n",
        "This formula allows you to update your belief in a hypothesis (A) based on new evidence (B) by taking into account both the prior probability of the hypothesis and the likelihood of the evidence under that hypothesis. It's a fundamental tool in Bayesian statistics and probabilistic reasoning.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "6SyBoqpw4Q-6",
        "outputId": "308c012f-cb24-4d3d-b869-4d0c93899256"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe formula for Bayes' theorem is as follows:\\n\\n\\\\[ P(A|B) = \\x0crac{P(B|A) \\\\cdot P(A)}{P(B)} \\\\]\\n\\nWhere:\\n\\n- \\\\(P(A|B)\\\\) is the posterior probability of hypothesis A given evidence B.\\n- \\\\(P(B|A)\\\\) is the probability of evidence B given hypothesis A.\\n- \\\\(P(A)\\\\) is the prior probability of hypothesis A (the probability of A before considering the evidence).\\n- \\\\(P(B)\\\\) is the probability of evidence B.\\n\\nThis formula allows you to update your belief in a hypothesis (A) based on new evidence (B) by taking into account both the prior probability of the hypothesis and the likelihood of the evidence under that hypothesis. It's a fundamental tool in Bayesian statistics and probabilistic reasoning.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. How is Bayes' theorem used in practice?\n",
        "\n",
        "'''\n",
        "Bayes' theorem is used in various practical applications across different fields. Here are some common ways in which Bayes' theorem is used in practice:\n",
        "\n",
        "1. **Medical Diagnosis**: Bayes' theorem is employed in medical diagnosis to assess the probability of a patient having a particular disease based on their symptoms, medical history, and test results. It helps doctors make more accurate diagnoses by updating the probability of a disease given the available evidence.\n",
        "\n",
        "2. **Spam Detection**: Email spam filters use Bayes' theorem to classify emails as either spam or not spam. They calculate the probability of an email being spam based on certain keywords and patterns in the email's content.\n",
        "\n",
        "3. **Machine Learning and Classification**: In machine learning, Bayesian methods, such as Naive Bayes classifiers, use Bayes' theorem to make predictions and classify data into different categories. These classifiers are particularly useful for text classification, sentiment analysis, and recommendation systems.\n",
        "\n",
        "4. **Financial Risk Assessment**: In finance, Bayes' theorem is used to assess the risk associated with various investments. It helps investors update their beliefs about the likelihood of different financial outcomes based on new information.\n",
        "\n",
        "5. **Natural Language Processing**: Bayes' theorem plays a role in various natural language processing tasks, including speech recognition, machine translation, and language modeling. It helps improve the accuracy of language models and predictive text algorithms.\n",
        "\n",
        "6. **Weather Forecasting**: Bayesian statistical methods are used in weather forecasting to update weather predictions based on real-time data, such as temperature, humidity, and atmospheric pressure.\n",
        "\n",
        "7. **A/B Testing**: In web analytics and marketing, Bayes' theorem can be used to analyze the results of A/B tests. It helps determine whether a change in a website or marketing strategy has a statistically significant impact on user behavior or conversion rates.\n",
        "\n",
        "8. **Robotics and Sensor Fusion**: In robotics, Bayes' theorem is applied to sensor fusion, where multiple sensors provide data about the robot's environment. It helps the robot make more accurate and robust decisions about its surroundings.\n",
        "\n",
        "9. **Epidemiology**: Epidemiologists use Bayes' theorem to estimate disease prevalence and assess the effectiveness of interventions, especially in the context of infectious disease outbreaks.\n",
        "\n",
        "10. **Fault Diagnosis**: In engineering and maintenance, Bayes' theorem is used for fault diagnosis in complex systems like machinery, vehicles, and industrial processes. It helps identify the most likely cause of a malfunction based on observed symptoms and historical data.\n",
        "\n",
        "These are just a few examples of how Bayes' theorem is applied in practice. Its flexibility and ability to update beliefs based on new evidence make it a valuable tool for making informed decisions and predictions in a wide range of fields.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "RZHkaW5F4XqL",
        "outputId": "0f3e996e-419a-4a67-8f9d-b1495a22232b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nBayes' theorem is used in various practical applications across different fields. Here are some common ways in which Bayes' theorem is used in practice:\\n\\n1. **Medical Diagnosis**: Bayes' theorem is employed in medical diagnosis to assess the probability of a patient having a particular disease based on their symptoms, medical history, and test results. It helps doctors make more accurate diagnoses by updating the probability of a disease given the available evidence.\\n\\n2. **Spam Detection**: Email spam filters use Bayes' theorem to classify emails as either spam or not spam. They calculate the probability of an email being spam based on certain keywords and patterns in the email's content.\\n\\n3. **Machine Learning and Classification**: In machine learning, Bayesian methods, such as Naive Bayes classifiers, use Bayes' theorem to make predictions and classify data into different categories. These classifiers are particularly useful for text classification, sentiment analysis, and recommendation systems.\\n\\n4. **Financial Risk Assessment**: In finance, Bayes' theorem is used to assess the risk associated with various investments. It helps investors update their beliefs about the likelihood of different financial outcomes based on new information.\\n\\n5. **Natural Language Processing**: Bayes' theorem plays a role in various natural language processing tasks, including speech recognition, machine translation, and language modeling. It helps improve the accuracy of language models and predictive text algorithms.\\n\\n6. **Weather Forecasting**: Bayesian statistical methods are used in weather forecasting to update weather predictions based on real-time data, such as temperature, humidity, and atmospheric pressure.\\n\\n7. **A/B Testing**: In web analytics and marketing, Bayes' theorem can be used to analyze the results of A/B tests. It helps determine whether a change in a website or marketing strategy has a statistically significant impact on user behavior or conversion rates.\\n\\n8. **Robotics and Sensor Fusion**: In robotics, Bayes' theorem is applied to sensor fusion, where multiple sensors provide data about the robot's environment. It helps the robot make more accurate and robust decisions about its surroundings.\\n\\n9. **Epidemiology**: Epidemiologists use Bayes' theorem to estimate disease prevalence and assess the effectiveness of interventions, especially in the context of infectious disease outbreaks.\\n\\n10. **Fault Diagnosis**: In engineering and maintenance, Bayes' theorem is used for fault diagnosis in complex systems like machinery, vehicles, and industrial processes. It helps identify the most likely cause of a malfunction based on observed symptoms and historical data.\\n\\nThese are just a few examples of how Bayes' theorem is applied in practice. Its flexibility and ability to update beliefs based on new evidence make it a valuable tool for making informed decisions and predictions in a wide range of fields.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
        "\n",
        "'''\n",
        "Bayes' theorem and conditional probability are closely related concepts in probability theory, and Bayes' theorem can be thought of as an extension of conditional probability. Conditional probability deals with the probability of an event occurring given that another event has already occurred, while Bayes' theorem specifically addresses how to update probabilities in light of new evidence.\n",
        "\n",
        "The relationship between Bayes' theorem and conditional probability can be understood through Bayes' theorem itself. Here's a breakdown of this relationship:\n",
        "\n",
        "1. **Conditional Probability**:\n",
        "   Conditional probability, denoted as \\(P(A|B)\\), represents the probability of event A occurring given that event B has occurred. Mathematically, it is defined as:\n",
        "\n",
        "   \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\]\n",
        "\n",
        "   In this formula:\n",
        "   - \\(P(A|B)\\) is the conditional probability of A given B.\n",
        "   - \\(P(A \\cap B)\\) is the joint probability of both A and B occurring.\n",
        "   - \\(P(B)\\) is the probability of event B occurring.\n",
        "\n",
        "2. **Bayes' Theorem**:\n",
        "   Bayes' theorem provides a way to update our belief in a hypothesis (event A) based on new evidence (event B). The formula is as follows:\n",
        "\n",
        "   \\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
        "\n",
        "   In Bayes' theorem:\n",
        "   - \\(P(A|B)\\) is the posterior probability of A given B, which is the updated probability of A after considering B.\n",
        "   - \\(P(B|A)\\) is the probability of B occurring given A, which represents how likely the evidence is under the hypothesis.\n",
        "   - \\(P(A)\\) is the prior probability of A, which is the initial probability of A before considering B.\n",
        "   - \\(P(B)\\) is the probability of B occurring, which acts as a normalization factor.\n",
        "\n",
        "The key relationship between Bayes' theorem and conditional probability is that Bayes' theorem uses conditional probabilities to update the probability of a hypothesis (A) based on observed evidence (B). It's a formalization of how conditional probabilities are combined with prior probabilities to make probabilistic inferences.\n",
        "\n",
        "In essence, conditional probability is a fundamental concept that underlies Bayes' theorem, and Bayes' theorem provides a structured framework for applying conditional probability to update beliefs or make predictions when new evidence becomes available.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "-CIvP6U84gFL",
        "outputId": "ff6efcd3-2ca1-4852-d592-34385871f07e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nBayes' theorem and conditional probability are closely related concepts in probability theory, and Bayes' theorem can be thought of as an extension of conditional probability. Conditional probability deals with the probability of an event occurring given that another event has already occurred, while Bayes' theorem specifically addresses how to update probabilities in light of new evidence.\\n\\nThe relationship between Bayes' theorem and conditional probability can be understood through Bayes' theorem itself. Here's a breakdown of this relationship:\\n\\n1. **Conditional Probability**:\\n   Conditional probability, denoted as \\\\(P(A|B)\\\\), represents the probability of event A occurring given that event B has occurred. Mathematically, it is defined as:\\n   \\n   \\\\[ P(A|B) = \\x0crac{P(A \\\\cap B)}{P(B)} \\\\]\\n   \\n   In this formula:\\n   - \\\\(P(A|B)\\\\) is the conditional probability of A given B.\\n   - \\\\(P(A \\\\cap B)\\\\) is the joint probability of both A and B occurring.\\n   - \\\\(P(B)\\\\) is the probability of event B occurring.\\n\\n2. **Bayes' Theorem**:\\n   Bayes' theorem provides a way to update our belief in a hypothesis (event A) based on new evidence (event B). The formula is as follows:\\n\\n   \\\\[ P(A|B) = \\x0crac{P(B|A) \\\\cdot P(A)}{P(B)} \\\\]\\n\\n   In Bayes' theorem:\\n   - \\\\(P(A|B)\\\\) is the posterior probability of A given B, which is the updated probability of A after considering B.\\n   - \\\\(P(B|A)\\\\) is the probability of B occurring given A, which represents how likely the evidence is under the hypothesis.\\n   - \\\\(P(A)\\\\) is the prior probability of A, which is the initial probability of A before considering B.\\n   - \\\\(P(B)\\\\) is the probability of B occurring, which acts as a normalization factor.\\n\\nThe key relationship between Bayes' theorem and conditional probability is that Bayes' theorem uses conditional probabilities to update the probability of a hypothesis (A) based on observed evidence (B). It's a formalization of how conditional probabilities are combined with prior probabilities to make probabilistic inferences.\\n\\nIn essence, conditional probability is a fundamental concept that underlies Bayes' theorem, and Bayes' theorem provides a structured framework for applying conditional probability to update beliefs or make predictions when new evidence becomes available.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
        "\n",
        "'''\n",
        "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on the nature of the data and the assumptions that can reasonably be made about the data. There are three common types of Naive Bayes classifiers:\n",
        "\n",
        "1. **Gaussian Naive Bayes**: This variant is suitable for problems where the features are continuous and follow a Gaussian (normal) distribution. It assumes that the data within each class is normally distributed. Gaussian Naive Bayes is often used in problems involving real-valued or continuous data, such as in some parts of natural language processing and image classification when dealing with numerical features like pixel values.\n",
        "\n",
        "2. **Multinomial Naive Bayes**: Multinomial Naive Bayes is used when the features represent discrete counts, such as word frequencies in text data. It's particularly common in text classification problems, like spam detection and document categorization, where the data is typically represented as a bag-of-words or term frequency matrix.\n",
        "\n",
        "3. **Bernoulli Naive Bayes**: This variant is suitable for binary or binarized data, where features represent binary variables (e.g., presence or absence of a term in a document). It's commonly used in text classification tasks when working with binary feature vectors, like binary document representations.\n",
        "\n",
        "Here are some considerations to help you choose the right type of Naive Bayes classifier:\n",
        "\n",
        "- **Nature of Features**: Consider the type of features you have in your dataset. Are they continuous, discrete, or binary? Choose the Naive Bayes variant that best matches the data type.\n",
        "\n",
        "- **Data Distribution Assumptions**: Assess whether the data distribution assumptions of the chosen Naive Bayes variant hold true for your dataset. For example, Gaussian Naive Bayes assumes normally distributed data, while Multinomial and Bernoulli Naive Bayes make different assumptions.\n",
        "\n",
        "- **Problem Domain**: Consider the specific problem domain and the characteristics of the data. For text classification tasks, Multinomial or Bernoulli Naive Bayes are often suitable due to the discrete nature of text data. For other types of data, Gaussian Naive Bayes may be appropriate.\n",
        "\n",
        "- **Data Preprocessing**: Preprocessing steps, such as feature engineering and transformation, can influence the choice of Naive Bayes classifier. For example, if you binarize your data, Bernoulli Naive Bayes might be a good choice.\n",
        "\n",
        "- **Model Performance**: Experiment with different Naive Bayes variants and evaluate their performance using appropriate metrics (e.g., accuracy, precision, recall, F1-score) on a validation set or through cross-validation. The variant that yields the best results on your specific problem should be chosen.\n",
        "\n",
        "- **Implementation and Libraries**: Consider the availability of libraries and implementations for each Naive Bayes variant in your programming language of choice. Some libraries may provide optimized versions of specific variants, making implementation easier.\n",
        "\n",
        "Ultimately, the choice of a Naive Bayes classifier should be based on a combination of the characteristics of your data and empirical performance on your specific problem. It's also worth noting that, despite its simplicity and the \"naive\" assumptions it makes, Naive Bayes can be surprisingly effective in many classification tasks, especially when dealing with high-dimensional data like text.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "q3BNEaN54uem",
        "outputId": "f9630d5f-aa83-4ddb-defb-d10f307845d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nChoosing the appropriate type of Naive Bayes classifier for a given problem depends on the nature of the data and the assumptions that can reasonably be made about the data. There are three common types of Naive Bayes classifiers:\\n\\n1. **Gaussian Naive Bayes**: This variant is suitable for problems where the features are continuous and follow a Gaussian (normal) distribution. It assumes that the data within each class is normally distributed. Gaussian Naive Bayes is often used in problems involving real-valued or continuous data, such as in some parts of natural language processing and image classification when dealing with numerical features like pixel values.\\n\\n2. **Multinomial Naive Bayes**: Multinomial Naive Bayes is used when the features represent discrete counts, such as word frequencies in text data. It\\'s particularly common in text classification problems, like spam detection and document categorization, where the data is typically represented as a bag-of-words or term frequency matrix.\\n\\n3. **Bernoulli Naive Bayes**: This variant is suitable for binary or binarized data, where features represent binary variables (e.g., presence or absence of a term in a document). It\\'s commonly used in text classification tasks when working with binary feature vectors, like binary document representations.\\n\\nHere are some considerations to help you choose the right type of Naive Bayes classifier:\\n\\n- **Nature of Features**: Consider the type of features you have in your dataset. Are they continuous, discrete, or binary? Choose the Naive Bayes variant that best matches the data type.\\n\\n- **Data Distribution Assumptions**: Assess whether the data distribution assumptions of the chosen Naive Bayes variant hold true for your dataset. For example, Gaussian Naive Bayes assumes normally distributed data, while Multinomial and Bernoulli Naive Bayes make different assumptions.\\n\\n- **Problem Domain**: Consider the specific problem domain and the characteristics of the data. For text classification tasks, Multinomial or Bernoulli Naive Bayes are often suitable due to the discrete nature of text data. For other types of data, Gaussian Naive Bayes may be appropriate.\\n\\n- **Data Preprocessing**: Preprocessing steps, such as feature engineering and transformation, can influence the choice of Naive Bayes classifier. For example, if you binarize your data, Bernoulli Naive Bayes might be a good choice.\\n\\n- **Model Performance**: Experiment with different Naive Bayes variants and evaluate their performance using appropriate metrics (e.g., accuracy, precision, recall, F1-score) on a validation set or through cross-validation. The variant that yields the best results on your specific problem should be chosen.\\n\\n- **Implementation and Libraries**: Consider the availability of libraries and implementations for each Naive Bayes variant in your programming language of choice. Some libraries may provide optimized versions of specific variants, making implementation easier.\\n\\nUltimately, the choice of a Naive Bayes classifier should be based on a combination of the characteristics of your data and empirical performance on your specific problem. It\\'s also worth noting that, despite its simplicity and the \"naive\" assumptions it makes, Naive Bayes can be surprisingly effective in many classification tasks, especially when dealing with high-dimensional data like text.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Assignment:\n",
        "# You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
        "# Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
        "# each feature value for each class:\n",
        "# Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
        "# A 3 3 4 4 3 3 3\n",
        "# B 2 2 1 2 2 2 3\n",
        "# Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
        "# to belong to?\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Given data\n",
        "X1_values = [1, 2, 3]\n",
        "X2_values = [1, 2, 3, 4]\n",
        "class_A_freq = np.array([[3, 3, 4, 4], [4, 3, 3, 3]])\n",
        "class_B_freq = np.array([[2, 2, 1, 2], [2, 2, 2, 3]])\n",
        "\n",
        "# New instance features\n",
        "new_X1 = 3\n",
        "new_X2 = 4\n",
        "\n",
        "# Calculate likelihood for class A\n",
        "likelihood_A = 1  # Initialize with 1 for multiplication\n",
        "for x1, x2 in zip(X1_values, X2_values):\n",
        "    if x1 == new_X1 and x2 == new_X2:\n",
        "        likelihood_A *= (class_A_freq[0, X1_values.index(x1)] / np.sum(class_A_freq[0])) * (class_A_freq[1, X2_values.index(x2)] / np.sum(class_A_freq[1]))\n",
        "\n",
        "# Calculate likelihood for class B\n",
        "likelihood_B = 1  # Initialize with 1 for multiplication\n",
        "for x1, x2 in zip(X1_values, X2_values):\n",
        "    if x1 == new_X1 and x2 == new_X2:\n",
        "        likelihood_B *= (class_B_freq[0, X1_values.index(x1)] / np.sum(class_B_freq[0])) * (class_B_freq[1, X2_values.index(x2)] / np.sum(class_B_freq[1]))\n",
        "\n",
        "# Compare likelihoods and make the prediction\n",
        "if likelihood_A > likelihood_B:\n",
        "    predicted_class = 'A'\n",
        "else:\n",
        "    predicted_class = 'B'\n",
        "\n",
        "print(f\"The Naive Bayes classifier predicts the new instance to belong to Class {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYPGM8RN43NR",
        "outputId": "d7144039-9b25-48b7-f728-e8c9116d9cc5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Naive Bayes classifier predicts the new instance to belong to Class B\n"
          ]
        }
      ]
    }
  ]
}