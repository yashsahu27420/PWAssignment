{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1F5O_vnNpEge",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0232af25-c913-404e-aeaa-2165b308ed13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe filter method filters out the irrelevant feature and redundant columns from the model by using different metrics through ranking.\\nThe advantage of using filter methods is that it needs low computational time and does not overfit the data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Q1. What is the Filter method in feature selection, and how does it work?\n",
        "'''\n",
        "The filter method filters out the irrelevant feature and redundant columns from the model by using different metrics through ranking.\n",
        "The advantage of using filter methods is that it needs low computational time and does not overfit the data.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. How does the Wrapper method differ from the Filter method in feature selection?\n",
        "'''\n",
        "The main differences between the filter and wrapper methods for feature selection are:\n",
        "Filter methods measure the relevance of features by their correlation with dependent variable while\n",
        "wrapper methods measure the usefulness of a subset of feature by actually training a model on it.'''"
      ],
      "metadata": {
        "id": "jb23Oc_-xs4J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "98a8e0a2-0074-4ccd-f5ec-8dbb88079c10"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe main differences between the filter and wrapper methods for feature selection are:\\nFilter methods measure the relevance of features by their correlation with dependent variable while\\nwrapper methods measure the usefulness of a subset of feature by actually training a model on it.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. What are some common techniques used in Embedded feature selection methods?\n",
        "'''\n",
        "The most Common embedded technique are the tree algorithm's like RandomForest, ExtraTree and so on.\n",
        "Tree algorithms select a feature in each recursive step of the tree growth process and divide the sample set into smaller subsets.'''"
      ],
      "metadata": {
        "id": "5TftTczOx94h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "91b2737b-f98f-416f-f95e-a4540a09ecc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe most Common embedded technique are the tree algorithm's like RandomForest, ExtraTree and so on.\\nTree algorithms select a feature in each recursive step of the tree growth process and divide the sample set into smaller subsets.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. What are some drawbacks of using the Filter method for feature selection?\n",
        "'''\n",
        "One drawback of such methods is that they do not interact with the predictive model for feature selection.\n",
        "Another drawback is seen in the case of univariate filter methods where dependencies between features are normally ignored '''"
      ],
      "metadata": {
        "id": "cc9XXKReydno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "84abc2a0-4daf-4e84-dbbd-16f8e01bc206"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOne drawback of such methods is that they do not interact with the predictive model for feature selection.\\nAnother drawback is seen in the case of univariate filter methods where dependencies between features are normally ignored '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?\n",
        "'''\n",
        "For large data you should use the Filter approaches because these approaches are rapid and for small size of data it is better to use Wrapper (KNN, SVM,...)\n",
        "approaches because they are slower than the Filter approaches. or you can combine the two approaches to have better results than the two approaches'''"
      ],
      "metadata": {
        "id": "mW8V-vrV2dby",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "36ac1900-42af-466b-f104-ffc7fe40c3a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFor large data you should use the Filter approaches because these approaches are rapid and for small size of data it is better to use Wrapper (KNN, SVM,...)\\napproaches because they are slower than the Filter approaches. or you can combine the two approaches to have better results than the two approaches'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
        "You are unsure of which features to include in the model because the dataset contains several different\n",
        "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.'''\n",
        "\n",
        "'''\n",
        "In a filter mode at first I choose the important features from the database.\n",
        "Then I correlatin of the selective features, then short and drop the unnecessary filters,\n",
        "then I am performing statistical tests like(Z-test,t-tsest,P-test,etc tests), so I should know how the data distribution shows in the features.\n",
        "Thren I finalize my ML test suitable features from the data base base on the 'target' or LABEL. '''"
      ],
      "metadata": {
        "id": "G7euSc7S2rwz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d4514a55-47a6-4750-fa84-0c12a04011e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nIn a filter mode at first I choose the important features from the database.\\nThen I correlatin of the selective features, then short and drop the unnecessary filters, \\nthen I am performing statistical tests like(Z-test,t-tsest,P-test,etc tests), so I should know how the data distribution shows in the features. \\nThren I finalize my ML test suitable features from the data base base on the 'target' or LABEL. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
        "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
        "method to select the most relevant features for the model.'''\n",
        "\n",
        "'''\n",
        "Embedded Methods\n",
        "Embedded methods combined the advantages of both filter and wrapper methods by considering the interaction of features along with low computational cost.\n",
        "These are fast processing methods similar to the filter method but more accurate than the filter method.\n",
        "\n",
        "Feature Selection Techniques in Machine Learning\n",
        "These methods are also iterative, which evaluates each iteration,\n",
        "and optimally finds the most important features that contribute the most to training in a particular iteration. Some techniques of embedded methods are:\n",
        "\n",
        "Regularization- Regularization adds a penalty term to different parameters of the machine learning model for avoiding overfitting in the model.\n",
        "This penalty term is added to the coefficients; hence it shrinks some coefficients to zero.\n",
        "Those features with zero coefficients can be removed from the dataset.\n",
        "The types of regularization techniques are L1 Regularization (Lasso Regularization) or Elastic Nets (L1 and L2 regularization).\n",
        "\n",
        "Random Forest Importance - Different tree-based methods of feature selection help us with feature importance to provide a way of selecting features.\n",
        "Here, feature importance specifies which feature has more importance in model building or has a great impact on the target variable.\n",
        "Random Forest is such a tree-based method, which is a type of bagging algorithm that aggregates a different number of decision trees.\n",
        "It automatically ranks the nodes by their performance or decrease in the impurity (Gini impurity) over all the trees.\n",
        "Nodes are arranged as per the impurity values, and thus it allows to pruning of trees below a specific node.\n",
        "The remaining nodes create a subset of the most important features.'''"
      ],
      "metadata": {
        "id": "9DIOQE6w3z77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "105acbb4-8ead-41a4-f27e-39950e6f919a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEmbedded Methods\\nEmbedded methods combined the advantages of both filter and wrapper methods by considering the interaction of features along with low computational cost. \\nThese are fast processing methods similar to the filter method but more accurate than the filter method.\\n\\nFeature Selection Techniques in Machine Learning\\nThese methods are also iterative, which evaluates each iteration, \\nand optimally finds the most important features that contribute the most to training in a particular iteration. Some techniques of embedded methods are:\\n\\nRegularization- Regularization adds a penalty term to different parameters of the machine learning model for avoiding overfitting in the model. \\nThis penalty term is added to the coefficients; hence it shrinks some coefficients to zero. \\nThose features with zero coefficients can be removed from the dataset. \\nThe types of regularization techniques are L1 Regularization (Lasso Regularization) or Elastic Nets (L1 and L2 regularization).\\n\\nRandom Forest Importance - Different tree-based methods of feature selection help us with feature importance to provide a way of selecting features. \\nHere, feature importance specifies which feature has more importance in model building or has a great impact on the target variable. \\nRandom Forest is such a tree-based method, which is a type of bagging algorithm that aggregates a different number of decision trees. \\nIt automatically ranks the nodes by their performance or decrease in the impurity (Gini impurity) over all the trees.\\nNodes are arranged as per the impurity values, and thus it allows to pruning of trees below a specific node. \\nThe remaining nodes create a subset of the most important features.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
        "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
        "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
        "predictor.'''\n",
        "\n",
        "'''In wrapper methodology, selection of features is done by considering it as a search problem, in which different combinations are made, evaluated, and compared with other combinations. It trains the algorithm by using the subset of features iteratively.\n",
        "\n",
        "Feature Selection Techniques in Machine Learning\n",
        "On the basis of the output of the model, features are added or subtracted, and with this feature set, the model has trained again.\n",
        "\n",
        "Some techniques of wrapper methods are:\n",
        "\n",
        "\n",
        "freestar\n",
        "Forward selection - Forward selection is an iterative process, which begins with an empty set of features.\n",
        "After each iteration, it keeps adding on a feature and evaluates the performance to check whether it is improving the performance or not.\n",
        "The process continues until the addition of a new variable/feature does not improve the performance of the model.\n",
        "\n",
        "Backward elimination - Backward elimination is also an iterative approach, but it is the opposite of forward selection.\n",
        "This technique begins the process by considering all the features and removes the least significant feature.\n",
        "This elimination process continues until removing the features does not improve the performance of the model.\n",
        "\n",
        "Exhaustive Feature Selection- Exhaustive feature selection is one of the best feature selection methods,\n",
        "which evaluates each feature set as brute-force.\n",
        "It means this method tries & make each possible combination of features and return the best performing feature set.\n",
        "\n",
        "Recursive Feature Elimination-\n",
        "Recursive feature elimination is a recursive greedy optimization approach,\n",
        "where features are selected by recursively taking a smaller and smaller subset of features.\n",
        "Now, an estimator is trained with each set of features, and the importance of each feature is determined using coef_attribute or through a feature_importances_attribute.'''"
      ],
      "metadata": {
        "id": "nB0kb5DF32Ni",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "4639215b-b726-45a8-9ffd-92e58423c2e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In wrapper methodology, selection of features is done by considering it as a search problem, in which different combinations are made, evaluated, and compared with other combinations. It trains the algorithm by using the subset of features iteratively.\\n\\nFeature Selection Techniques in Machine Learning\\nOn the basis of the output of the model, features are added or subtracted, and with this feature set, the model has trained again.\\n\\nSome techniques of wrapper methods are:\\n\\n\\nfreestar\\nForward selection - Forward selection is an iterative process, which begins with an empty set of features. \\nAfter each iteration, it keeps adding on a feature and evaluates the performance to check whether it is improving the performance or not. \\nThe process continues until the addition of a new variable/feature does not improve the performance of the model.\\n\\nBackward elimination - Backward elimination is also an iterative approach, but it is the opposite of forward selection. \\nThis technique begins the process by considering all the features and removes the least significant feature. \\nThis elimination process continues until removing the features does not improve the performance of the model.\\n\\nExhaustive Feature Selection- Exhaustive feature selection is one of the best feature selection methods, \\nwhich evaluates each feature set as brute-force. \\nIt means this method tries & make each possible combination of features and return the best performing feature set.\\n\\nRecursive Feature Elimination-\\nRecursive feature elimination is a recursive greedy optimization approach, \\nwhere features are selected by recursively taking a smaller and smaller subset of features. \\nNow, an estimator is trained with each set of features, and the importance of each feature is determined using coef_attribute or through a feature_importances_attribute.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}