{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "AQmLwi6JI2gh",
        "outputId": "29e7ff10-b19d-4eba-b7af-8ae9d938a1f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTo find the probability that an employee is a smoker given that they use the health insurance plan, you can use conditional probability. You can use the formula for conditional probability:\\n\\n\\\\[P(A|B) = \\x0crac{P(A \\\\cap B)}{P(B)}\\\\]\\n\\nIn this case:\\n- \\\\(P(A)\\\\) is the probability that an employee is a smoker.\\n- \\\\(P(B)\\\\) is the probability that an employee uses the health insurance plan.\\n- \\\\(P(A \\\\cap B)\\\\) is the probability that an employee is both a smoker and uses the health insurance plan.\\n\\nFrom the information given:\\n- \\\\(P(B)\\\\), the probability that an employee uses the health insurance plan, is 70% or 0.70.\\n- \\\\(P(A|B)\\\\), the probability that an employee is a smoker given that they use the health insurance plan, is what we want to find.\\n- \\\\(P(A)\\\\), the probability that an employee is a smoker, is 40% of the employees who use the plan, which is 0.40.\\n\\nNow, you can plug these values into the formula:\\n\\n\\\\[P(A|B) = \\x0crac{P(A \\\\cap B)}{P(B)} = \\x0crac{0.40}{0.70} \\x07pprox 0.5714\\\\]\\n\\nSo, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.5714 or 57.14%.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
        "# company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
        "# probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "\n",
        "'''\n",
        "To find the probability that an employee is a smoker given that they use the health insurance plan, you can use conditional probability. You can use the formula for conditional probability:\n",
        "\n",
        "\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n",
        "\n",
        "In this case:\n",
        "- \\(P(A)\\) is the probability that an employee is a smoker.\n",
        "- \\(P(B)\\) is the probability that an employee uses the health insurance plan.\n",
        "- \\(P(A \\cap B)\\) is the probability that an employee is both a smoker and uses the health insurance plan.\n",
        "\n",
        "From the information given:\n",
        "- \\(P(B)\\), the probability that an employee uses the health insurance plan, is 70% or 0.70.\n",
        "- \\(P(A|B)\\), the probability that an employee is a smoker given that they use the health insurance plan, is what we want to find.\n",
        "- \\(P(A)\\), the probability that an employee is a smoker, is 40% of the employees who use the plan, which is 0.40.\n",
        "\n",
        "Now, you can plug these values into the formula:\n",
        "\n",
        "\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{0.40}{0.70} \\approx 0.5714\\]\n",
        "\n",
        "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.5714 or 57.14%.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "'''\n",
        "Bernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of the Naive Bayes classification algorithm used in machine learning. They are primarily used for text classification and have some key differences:\n",
        "\n",
        "1. **Nature of Input Data**:\n",
        "   - **Bernoulli Naive Bayes:** It is typically used for binary feature data, where each feature can be either present (1) or absent (0). It is suitable for problems where you want to classify data into two categories.\n",
        "   - **Multinomial Naive Bayes:** It is used for categorical or count-based data, where each feature represents the frequency of a term or word in a document. It is commonly used in text classification where you have multiple classes (more than two) to predict.\n",
        "\n",
        "2. **Representation of Data**:\n",
        "   - **Bernoulli Naive Bayes:** It works with binary feature vectors, often called \"presence-absence\" vectors. It models whether a feature is present or not.\n",
        "   - **Multinomial Naive Bayes:** It works with count-based feature vectors, where each feature represents the number of occurrences of a term in a document.\n",
        "\n",
        "3. **Probability Estimation**:\n",
        "   - **Bernoulli Naive Bayes:** It estimates probabilities using the presence or absence of features. It typically uses the Bernoulli distribution to model the likelihood of features.\n",
        "   - **Multinomial Naive Bayes:** It estimates probabilities based on the frequency of features. It uses the multinomial distribution to model the likelihood of features.\n",
        "\n",
        "4. **Use Cases**:\n",
        "   - **Bernoulli Naive Bayes:** It is commonly used for tasks like spam email classification, sentiment analysis (positive or negative sentiment), or any binary classification problem where features are binary indicators.\n",
        "   - **Multinomial Naive Bayes:** It is well-suited for text classification tasks with multiple categories, such as document categorization, topic classification, or language classification.\n",
        "\n",
        "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your data and the specific classification problem you are trying to solve. If your data consists of binary features and you're dealing with a binary classification problem, Bernoulli Naive Bayes may be more appropriate. If you have count-based data and are dealing with multi-class classification problems, Multinomial Naive Bayes is often a better choice.'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "J15IprW-JEdP",
        "outputId": "5e1c381f-3f9a-4045-8e48-b90112abc838"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of the Naive Bayes classification algorithm used in machine learning. They are primarily used for text classification and have some key differences:\\n\\n1. **Nature of Input Data**:\\n   - **Bernoulli Naive Bayes:** It is typically used for binary feature data, where each feature can be either present (1) or absent (0). It is suitable for problems where you want to classify data into two categories.\\n   - **Multinomial Naive Bayes:** It is used for categorical or count-based data, where each feature represents the frequency of a term or word in a document. It is commonly used in text classification where you have multiple classes (more than two) to predict.\\n\\n2. **Representation of Data**:\\n   - **Bernoulli Naive Bayes:** It works with binary feature vectors, often called \"presence-absence\" vectors. It models whether a feature is present or not.\\n   - **Multinomial Naive Bayes:** It works with count-based feature vectors, where each feature represents the number of occurrences of a term in a document.\\n\\n3. **Probability Estimation**:\\n   - **Bernoulli Naive Bayes:** It estimates probabilities using the presence or absence of features. It typically uses the Bernoulli distribution to model the likelihood of features.\\n   - **Multinomial Naive Bayes:** It estimates probabilities based on the frequency of features. It uses the multinomial distribution to model the likelihood of features.\\n\\n4. **Use Cases**:\\n   - **Bernoulli Naive Bayes:** It is commonly used for tasks like spam email classification, sentiment analysis (positive or negative sentiment), or any binary classification problem where features are binary indicators.\\n   - **Multinomial Naive Bayes:** It is well-suited for text classification tasks with multiple categories, such as document categorization, topic classification, or language classification.\\n\\nIn summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your data and the specific classification problem you are trying to solve. If your data consists of binary features and you\\'re dealing with a binary classification problem, Bernoulli Naive Bayes may be more appropriate. If you have count-based data and are dealing with multi-class classification problems, Multinomial Naive Bayes is often a better choice.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "'''\n",
        "Bernoulli Naive Bayes, like other Naive Bayes variants, handles missing values in a straightforward manner. When dealing with missing values in a Bernoulli Naive Bayes model, you typically have two options:\n",
        "\n",
        "1. **Ignore Missing Values:** One common approach is to simply ignore instances with missing values during the training and classification process. In Bernoulli Naive Bayes, features are treated as binary (presence or absence), so if a feature is missing for a particular instance, you can treat it as if the feature is absent. This means that the missing feature is considered as a \"0\" in the feature vector for that instance.\n",
        "\n",
        "2. **Impute Missing Values:** Alternatively, you can choose to impute or fill in the missing values before training your Bernoulli Naive Bayes model. There are various imputation methods available, such as replacing missing values with the mode (most frequent value) of that feature or using more sophisticated imputation techniques if your data allows. Once missing values are imputed, you can proceed with training and classification as usual.\n",
        "\n",
        "The choice between these two approaches depends on the nature of your data and the impact of missing values on the quality of your model. Here are some considerations:\n",
        "\n",
        "- Ignoring missing values is simpler and may be appropriate when missing values are relatively rare and not systematically related to the class labels.\n",
        "\n",
        "- Imputing missing values may be necessary when missingness is informative or when missing values are prevalent in your dataset. Imputation can help preserve the information that might otherwise be lost if you simply ignore the instances with missing values.\n",
        "\n",
        "Remember that the \"naive\" assumption in Naive Bayes is that features are conditionally independent given the class label. This assumption can be compromised when dealing with missing values, so handling them appropriately is essential to maintain the validity of the model. The choice of how to handle missing values should be based on the specifics of your dataset and the goals of your analysis.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "9bBJecT5JiSW",
        "outputId": "6b145aa4-6767-4f65-9103-3d876128940d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBernoulli Naive Bayes, like other Naive Bayes variants, handles missing values in a straightforward manner. When dealing with missing values in a Bernoulli Naive Bayes model, you typically have two options:\\n\\n1. **Ignore Missing Values:** One common approach is to simply ignore instances with missing values during the training and classification process. In Bernoulli Naive Bayes, features are treated as binary (presence or absence), so if a feature is missing for a particular instance, you can treat it as if the feature is absent. This means that the missing feature is considered as a \"0\" in the feature vector for that instance.\\n\\n2. **Impute Missing Values:** Alternatively, you can choose to impute or fill in the missing values before training your Bernoulli Naive Bayes model. There are various imputation methods available, such as replacing missing values with the mode (most frequent value) of that feature or using more sophisticated imputation techniques if your data allows. Once missing values are imputed, you can proceed with training and classification as usual.\\n\\nThe choice between these two approaches depends on the nature of your data and the impact of missing values on the quality of your model. Here are some considerations:\\n\\n- Ignoring missing values is simpler and may be appropriate when missing values are relatively rare and not systematically related to the class labels.\\n\\n- Imputing missing values may be necessary when missingness is informative or when missing values are prevalent in your dataset. Imputation can help preserve the information that might otherwise be lost if you simply ignore the instances with missing values.\\n\\nRemember that the \"naive\" assumption in Naive Bayes is that features are conditionally independent given the class label. This assumption can be compromised when dealing with missing values, so handling them appropriately is essential to maintain the validity of the model. The choice of how to handle missing values should be based on the specifics of your dataset and the goals of your analysis.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "\n",
        "'''\n",
        "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that the continuous-valued features of the data follow a Gaussian (normal) distribution. It's often used for classification tasks where the feature distribution of each class is assumed to be Gaussian.\n",
        "\n",
        "To perform multi-class classification using Gaussian Naive Bayes, you can follow these steps:\n",
        "\n",
        "1. **Data Preparation:** Ensure that your dataset includes features that are continuous and approximately follow a Gaussian distribution for each class. You should also have labeled samples for each class.\n",
        "\n",
        "2. **Training:** Estimate the parameters of the Gaussian distribution (mean and variance) for each feature in each class. This involves calculating the mean and variance of each feature for each class in the training data.\n",
        "\n",
        "3. **Classification:** When you want to classify a new data point into one of the multiple classes, you calculate the likelihood of the data point belonging to each class based on the Gaussian distribution parameters learned during training. You also calculate the prior probabilities of each class. Then, you apply Bayes' theorem to compute the posterior probabilities for each class and choose the class with the highest posterior probability as the predicted class for the new data point.\n",
        "\n",
        "4. **Decision Rule:** The decision rule for multi-class Gaussian Naive Bayes typically involves selecting the class with the maximum posterior probability:\n",
        "\n",
        "   \\[ \\text{Predicted Class} = \\arg\\max_{c} P(C=c | X) \\]\n",
        "\n",
        "   Where:\n",
        "   - \\(C\\) represents the class variable.\n",
        "   - \\(X\\) represents the feature vector of the new data point.\n",
        "   - \\(P(C=c | X)\\) is the posterior probability of class \\(c\\) given the features \\(X\\).\n",
        "\n",
        "It's important to note that while Gaussian Naive Bayes can be used for multi-class classification, it assumes that the features are continuous and Gaussian-distributed within each class. If this assumption does not hold well for your data, other classification algorithms may be more suitable. Additionally, Naive Bayes assumes feature independence, which may not always be true in practice, so its performance can vary depending on the dataset and the degree of feature independence.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "CcfQlLv5J1jn",
        "outputId": "01e7a103-9b85-4bd0-b02c-4d6c250a88e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nYes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that the continuous-valued features of the data follow a Gaussian (normal) distribution. It's often used for classification tasks where the feature distribution of each class is assumed to be Gaussian.\\n\\nTo perform multi-class classification using Gaussian Naive Bayes, you can follow these steps:\\n\\n1. **Data Preparation:** Ensure that your dataset includes features that are continuous and approximately follow a Gaussian distribution for each class. You should also have labeled samples for each class.\\n\\n2. **Training:** Estimate the parameters of the Gaussian distribution (mean and variance) for each feature in each class. This involves calculating the mean and variance of each feature for each class in the training data.\\n\\n3. **Classification:** When you want to classify a new data point into one of the multiple classes, you calculate the likelihood of the data point belonging to each class based on the Gaussian distribution parameters learned during training. You also calculate the prior probabilities of each class. Then, you apply Bayes' theorem to compute the posterior probabilities for each class and choose the class with the highest posterior probability as the predicted class for the new data point.\\n\\n4. **Decision Rule:** The decision rule for multi-class Gaussian Naive Bayes typically involves selecting the class with the maximum posterior probability:\\n\\n   \\\\[ \\text{Predicted Class} = \\x07rg\\\\max_{c} P(C=c | X) \\\\]\\n\\n   Where:\\n   - \\\\(C\\\\) represents the class variable.\\n   - \\\\(X\\\\) represents the feature vector of the new data point.\\n   - \\\\(P(C=c | X)\\\\) is the posterior probability of class \\\\(c\\\\) given the features \\\\(X\\\\).\\n\\nIt's important to note that while Gaussian Naive Bayes can be used for multi-class classification, it assumes that the features are continuous and Gaussian-distributed within each class. If this assumption does not hold well for your data, other classification algorithms may be more suitable. Additionally, Naive Bayes assumes feature independence, which may not always be true in practice, so its performance can vary depending on the dataset and the degree of feature independence.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. Assignment:\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "# Assign column names to the dataset (column names are available in the dataset description)\n",
        "data.columns = [\n",
        "    f\"word_freq_{i}\" for i in range(48)\n",
        "] + [\n",
        "    f\"char_freq_{i}\" for i in range(6)\n",
        "] + [\n",
        "    \"capital_run_length_average\",\n",
        "    \"capital_run_length_longest\",\n",
        "    \"capital_run_length_total\",\n",
        "    \"is_spam\"\n",
        "]\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop(\"is_spam\", axis=1)\n",
        "y = data[\"is_spam\"]\n",
        "\n",
        "# Split the data into training and testing sets (you can adjust the test_size as needed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Naive Bayes classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Perform 10-fold cross-validation and evaluate the classifiers\n",
        "def evaluate_classifier(classifier, name):\n",
        "    scores = cross_val_score(classifier, X_train, y_train, cv=10, scoring='accuracy')\n",
        "    print(f\"{name} Classifier:\")\n",
        "    print(f\"Cross-validation Accuracy: {scores.mean():.4f}\")\n",
        "\n",
        "evaluate_classifier(bernoulli_nb, \"Bernoulli\")\n",
        "evaluate_classifier(multinomial_nb, \"Multinomial\")\n",
        "evaluate_classifier(gaussian_nb, \"Gaussian\")\n",
        "\n",
        "# Train each classifier on the entire training dataset\n",
        "bernoulli_nb.fit(X_train, y_train)\n",
        "multinomial_nb.fit(X_train, y_train)\n",
        "gaussian_nb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate classifiers on the test set and report performance metrics\n",
        "def evaluate_test_set(classifier, name):\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    report = classification_report(y_test, y_pred, target_names=[\"Not Spam\", \"Spam\"])\n",
        "    print(f\"{name} Classifier Test Set Performance:\\n{report}\")\n",
        "\n",
        "evaluate_test_set(bernoulli_nb, \"Bernoulli\")\n",
        "evaluate_test_set(multinomial_nb, \"Multinomial\")\n",
        "evaluate_test_set(gaussian_nb, \"Gaussian\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR5dH9aDKQaZ",
        "outputId": "57764c01-bd08-493a-cfbc-6dc92d04ad59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli Classifier:\n",
            "Cross-validation Accuracy: 0.8873\n",
            "Multinomial Classifier:\n",
            "Cross-validation Accuracy: 0.7907\n",
            "Gaussian Classifier:\n",
            "Cross-validation Accuracy: 0.8106\n",
            "Bernoulli Classifier Test Set Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Not Spam       0.87      0.93      0.90       804\n",
            "        Spam       0.89      0.81      0.85       577\n",
            "\n",
            "    accuracy                           0.88      1381\n",
            "   macro avg       0.88      0.87      0.87      1381\n",
            "weighted avg       0.88      0.88      0.88      1381\n",
            "\n",
            "Multinomial Classifier Test Set Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Not Spam       0.79      0.84      0.82       804\n",
            "        Spam       0.76      0.69      0.73       577\n",
            "\n",
            "    accuracy                           0.78      1381\n",
            "   macro avg       0.78      0.77      0.77      1381\n",
            "weighted avg       0.78      0.78      0.78      1381\n",
            "\n",
            "Gaussian Classifier Test Set Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Not Spam       0.95      0.74      0.83       804\n",
            "        Spam       0.72      0.95      0.82       577\n",
            "\n",
            "    accuracy                           0.82      1381\n",
            "   macro avg       0.84      0.84      0.82      1381\n",
            "weighted avg       0.86      0.82      0.83      1381\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCRNrN6RMEPn"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}