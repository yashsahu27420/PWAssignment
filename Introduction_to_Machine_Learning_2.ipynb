{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONoWg7JyD9H1H2/QQh7zfP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKLguFn0Vuat"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Overfitting in machine learning is like memorizing a textbook instead of understanding concepts. It happens when a model learns the training data too well but struggles with new data. Underfitting, on the other hand, is like not studying enough—it fails to grasp the training data. Both lead to poor predictions. To fix, balance the complexity of the model.\n",
        "\n",
        "Q2: To reduce overfitting, think of your model like Goldilocks finding the right porridge. Don't make it too simple (underfitting) or too complex (overfitting). Adjust your model's complexity by using techniques like regularization or adding more data. This helps it generalize well beyond the training set.\n",
        "\n",
        "Q3: Underfitting is when your model is like a student who didn't pay attention in class—it doesn't understand the training data. It occurs when the model is too simple to capture the underlying patterns. This might happen if you use a basic algorithm for a complex problem or don't provide enough data.\n",
        "\n",
        "Q4: Imagine shooting arrows at a target. Bias is how far your shots are from the bullseye, and variance is how spread out they are. The bias-variance tradeoff in machine learning is finding the sweet spot: too much bias (underfitting) or too much variance (overfitting) both harm performance. Balancing them improves accuracy.\n",
        "\n",
        "Q5: Detecting overfitting is like checking if a chef only knows one recipe too well. Use methods like cross-validation or monitoring performance on a separate validation set. For underfitting, watch if your model struggles even with the training data. If predictions are way off, your model might be too simple.\n",
        "\n",
        "Q6: Bias and variance are like a seesaw. High bias is when your model consistently gets predictions wrong (like always saying it will rain). High variance is when it's too sensitive to small changes (like predicting rain when there's a tiny cloud). Finding balance is key for good performance.\n",
        "\n",
        "Q7: Regularization in machine learning is like adding speed bumps on a road to prevent speeding. It helps prevent overfitting by penalizing complex models. Common techniques include L1 and L2 regularization, acting like brakes for the model's complexity. They make sure the model doesn't go too fast and stays on track."
      ],
      "metadata": {
        "id": "gZ1Sl5LvWBey"
      }
    }
  ]
}